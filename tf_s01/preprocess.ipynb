{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "\n",
    "# Code for loading / extracting features from raw data \n",
    "\n",
    "############################################################\n",
    "\n",
    "# import h5py\n",
    "import librosa \n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm import tqdm \n",
    "from glob import glob \n",
    "\n",
    "# Save list data \n",
    "def save_list(path, data):\n",
    "    with open(path, 'wb') as f:\n",
    "        pkl.dump(data, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load list data \n",
    "def load_list(path): \n",
    "    with open(path, 'rb') as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Loading ToyTrain ####################\n",
      "Found file counts : 990\n",
      "First raw dataset path : ../data/unziped/ToyTrain/train/section_00_source_train_normal_0889_noAttribute.wav\n",
      "==================== Loading data ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 1495.86it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1673664.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Cropping features ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 42365.84it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 52137.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "is_test = True \n",
    "dataset_type = \"train\"\n",
    "domain = ['source', 'target']\n",
    "crop_sec = 4\n",
    "padding = 1\n",
    "to_feature = \"mfcc\"\n",
    "# path = glob(\"../data/unziped/*\")\n",
    "class_names = ['ToyTrain', 'gearbox', 'ToyCar', 'bearing', 'valve', 'fan', 'slider']\n",
    "labels = [\"normal\", \"anomaly\"]\n",
    "\n",
    "if is_test == True: dataset_type == \"test\"\n",
    "\n",
    "\n",
    "print(\"#\"*20, \"Loading\", class_names[0], \"#\"*20)\n",
    "# Read path\n",
    "path = glob(\"../data/unziped/*\" + class_names[0] + \"/\" + dataset_type + \"/*.wav\")\n",
    "\n",
    "# Check domain \n",
    "path = [i for i in path if i.split(\"_\")[2] == domain[0]]\n",
    "\n",
    "# Check for empty list and things ...\n",
    "print(\"Found file counts :\", len(path))\n",
    "\n",
    "if len(path) == 0: \n",
    "    print(\"#\"*20, \"ERROR\", \"#\"*20)\n",
    "    print(\"No file has been found\")\n",
    "    print(\"#\"*20, \"ERROR\", \"#\"*20)\n",
    "    # return None\n",
    "\n",
    "print(\"First raw dataset path :\", path[0])\n",
    "\n",
    "\n",
    "# Loading data [audio_wav, classes, domain]\n",
    "print(\"=\"*20, \"Loading data\", \"=\"*20)\n",
    "raw_data = [[librosa.load(i, sr=16e3)[0], i.split(\"/\")[3], i.split(\"_\")[2], i.split(\"_\")[4]] for i in tqdm(path)]\n",
    "\n",
    "# label and domain encoding to int [audio_feature, machine_type, source, label]\n",
    "audio_data = [[i[0], class_names.index(i[1]), domain.index(i[2]), 0] if i[3] == \"normal\" else [i[0], class_names.index(i[1]), domain.index(i[2]), 1] for i in tqdm(raw_data)]\n",
    "\n",
    "# Cropping features to \"crop_sec\" + augment feature\n",
    "print(\"=\"*20, \"Cropping features\", \"=\"*20)\n",
    "audio_data_one = [[np.array(i[0][int(16e3):int(16e3) * (crop_sec + 1)]), i[1], i[2], i[3]] for i in tqdm(audio_data)] \n",
    "audio_data_two = [[np.array(i[0][int(16e3) * (crop_sec + 2):int(16e3) * (crop_sec * 2 + 1)]), i[1], i[2], i[3]] for i in tqdm(audio_data)] \n",
    "audio_data = audio_data_one + audio_data_two\n",
    "print(len(audio_data))\n",
    "\n",
    "# print(\"=\"* 20, \"DEBUG\", \"=\"* 20)\n",
    "# print(audio_data[0])\n",
    "# print(\"=\"* 20, \"DEBUG\", \"=\"* 20)\n",
    "\n",
    "# # Extracting features\n",
    "# print(\"=\"*20, \"Extracting features\", \"=\"*20) \n",
    "# feature_data = [[librosa.feature.mfcc(y = i[0], sr = 16e3, n_mfcc=128,), i[1], i[2], i[3]] for i in tqdm(audio_data)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Loading ToyTrain ####################\n",
      "Found file counts : 990\n",
      "First raw dataset path : ../data/unziped/ToyTrain/train/section_00_source_train_normal_0889_noAttribute.wav\n",
      "==================== Loading data ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 1501.61it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 1782894.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Cropping features ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990/990 [00:00<00:00, 11562.05it/s]\n",
      "100%|██████████| 990/990 [00:00<00:00, 11564.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.00500488, -0.00204468,  0.00280762, ..., -0.00534058,\n",
       "        -0.01669312,  0.00213623], dtype=float32),\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([i[1] for i in tmp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
